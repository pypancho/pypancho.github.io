{"version":3,"file":"component---src-pages-index-old-js-550796386d2c7103a1d8.js","mappings":"sIAGMA,EAAa,CACjBC,MAAO,UACPC,WAAY,0CAIRC,EAAQ,CACZ,CACEC,OAAQ,2BAAK,wBAAMC,UAAU,aAAhB,YAAL,kFACRC,MAAO,kHACPC,OAAQ,+CACRC,MAAO,oCACPC,IAAK,yGACLC,SAAU,muCAEX,CACCN,OAAQ,2BAAK,wBAAMC,UAAU,aAAhB,YAAL,sFACRC,MAAO,4IACPC,OAAQ,+CACRC,MAAO,oCACPC,IAAK,yGACLC,SAAU,6pCAEZ,CACEN,OAAQ,sDAA6B,wBAAMC,UAAU,aAAhB,YAA7B,yEACRC,MAAO,yGACPC,OAAQ,+BACRC,MAAO,uCACPC,IAAK,iHACLC,SAAU,yiCAEZ,CACEN,OAAQ,2BAAK,wBAAMC,UAAU,aAAhB,YAAL,0HACRC,MAAO,yGACPC,OAAQ,2CACRC,MAAO,+CACPC,IAAK,yEACLC,SAAU,6yCAEZ,CACEN,OAAQ,2BAAK,wBAAMC,UAAU,aAAhB,YAAL,gGACRC,MAAO,2JACPC,OAAQ,kCACRC,MAAO,iDACPC,IAAK,yEACLC,SAAU,o8CAEZ,CACEN,OAAQ,0CAAiB,wBAAMC,UAAU,aAAhB,YAAjB,+BACRC,MAAO,iDACPC,OAAQ,qCACRC,MAAO,+CACPC,IAAK,iJACLC,SAAU,o2BAqJd,UAhJkB,WAChB,OACE,wBAAMC,MAAOX,GACX,uBAAKK,UAAU,yCACb,uBAAKA,UAAU,gDACb,uBAAKA,UAAU,8DACb,uBAAKA,UAAU,kBACb,qBAAGO,KAAK,QAAR,aAEF,uBAAKP,UAAU,8CACb,qBAAGO,KAAK,aAAR,uBAEF,uBAAKP,UAAU,8CACb,qBAAGO,KAAK,SAAR,SAEF,uBAAKP,UAAU,8CACb,qBAAGO,KAAK,eAAR,eAEF,uBAAKP,UAAU,8CACb,qBAAGO,KAAK,iBAAR,iBAEF,uBAAKP,UAAU,8CACb,qBAAGO,KAAK,aAAR,4BAKR,uBAAKC,GAAG,QACR,2BACE,uBAAKR,UAAU,+DACb,uBAAKA,UAAU,2BACb,2BACE,uBAAKA,UAAU,iBAAiBS,IAAI,wEAEtC,uBAAKT,UAAU,4BACb,qBAAGA,UAAU,yBAAb,oWAGA,qBAAGA,UAAU,QAAb,wCACqC,wDADrC,uEAC2I,uCAD3I,kHAC2Q,uCAD3Q,sHAGA,uBAAKA,UAAU,8BACb,2BAAK,wBAAMA,UAAU,aAAhB,YAAL,2FAAwI,wBAAMA,UAAU,aAAhB,UAAxI,2BAUN,uBAAKQ,GAAG,YAAYR,UAAU,SAC5B,uBAAKA,UAAU,mCAAf,sBACA,qBAAGA,UAAU,WAAb,kJACA,uBAAKA,UAAU,sBACb,sBAAIA,UAAU,aACZ,mDACA,8CACA,kDACA,iDACA,oDACA,4CAQN,uBAAKQ,GAAG,QACN,uBAAKR,UAAU,mCAAf,QACA,uBAAKA,UAAU,sBACb,sBAAIA,UAAU,aACZ,4CAAmB,iCAAnB,+CACA,6HACA,wIACA,sEACA,6GACA,iFACA,yHACA,oGACA,wGAKN,uBAAKQ,GAAG,cACN,uBAAKR,UAAU,mCAAf,cACA,uBAAKA,UAAU,sBACb,sBAAIA,UAAU,aACZ,+GACA,mIACA,+GACA,+GAKN,uBAAKQ,GAAG,gBACN,uBAAKR,UAAU,mCAAf,uBACA,uBAAKA,UAAU,QACZF,EAAMY,KAAI,SAACC,EAAMH,GAAP,OACT,uBAAKI,IAAKJ,EAAIR,UAAU,6CACtB,uBAAKA,UAAU,kBACb,uBAAKA,UAAU,iBAAiBS,IAAKE,EAAKP,OAE5C,uBAAKJ,UAAU,kBACb,2BAAMW,EAAKZ,QACX,uBAAKC,UAAU,aAAaW,EAAKV,OACjC,2BAAMU,EAAKT,QACX,2BACE,qBAAGF,UAAU,wBAAwBO,KAAMI,EAAKR,MAAOU,OAAO,UAA9D,gBAGJ,uBAAKb,UAAU,8BACZW,EAAKN,gBAOhB,uBAAKG,GAAG,YACN,uBAAKR,UAAU,mCAAf,wBACA,uBAAKA,UAAU,sBACb,sBAAIA,UAAU,aACZ,+MACA,8PACA,mJAMR,uBAAKA,UAAU,4DACb,mEACA","sources":["webpack://pangyan-blog/./src/pages/index_old.js"],"sourcesContent":["import * as React from \"react\"\n\n// styles\nconst pageStyles = {\n  color: \"#232129\",\n  fontFamily: \"Georgia, Times New Roman, Times, serif\",\n}\n\n// data\nconst links = [\n  {\n    author: <div><span className=\"font-bold\">Yan Pang</span>, Ai Shan, Zhen Wang, Mengyu Wang, Jianwei Li, Ji Zhang, Teng Huang, Chao Liu </div>,\n    title: \"Sparse-Dyn: Sparse dynamic graph multirepresentation learning via event-based sparse temporal attention network\",\n    jounal: \"International Journal of Intelligent Systems\",\n    paper: \"https://doi.org/10.1002/int.22967\",\n    img: \"https://onlinelibrary.wiley.com/cms/asset/00a3448e-89d1-4e24-9bc8-c3075a20b7e5/int22967-fig-0001-m.jpg\",\n    abstract: \"Dynamic graph neural networks (DGNNs) have been widely used in modeling and representation learning of graph structure data. Current dynamic representation learning focuses on either discrete learning which results in temporal information loss, or continuous learning which involves heavy computation. In this study, we proposed a novel DGNN, sparse dynamic (Sparse-Dyn). It adaptively encodes temporal information into a sequence of patches with an equal amount of temporal-topological structure. Therefore, while avoiding using snapshots which cause information loss, it also achieves a finer time granularity, which is close to what continuous networks could provide. In addition, we also designed a lightweight module, Sparse Temporal Transformer, to compute node representations through structural neighborhoods and temporal dynamics. Since the fully connected attention conjunction is simplified, the computation cost is far lower than the current state-of-the-art. Link prediction experiments are conducted on both continuous and discrete graph data sets. By comparing several state-of-the-art graph embedding baselines, the experimental results demonstrate that Sparse-Dyn has a faster inference speed while having competitive performance.\",\n   },\n   {\n    author: <div><span className=\"font-bold\">Yan Pang</span>, Teng Huang, Zhen Wang, Jianwei Li, Poorya Hosseini, Ji Zhang, Chao Liu, Shan Ai </div>,\n    title: \"Graph Decipher: A transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification\",\n    jounal: \"International Journal of Intelligent Systems\",\n    paper: \"https://doi.org/10.1002/int.22966\",\n    img: \"https://onlinelibrary.wiley.com/cms/asset/18dc386e-138f-4293-a5e9-aa30d31459f4/int22966-fig-0001-m.jpg\",\n    abstract: \"Graph neural networks (GNNs) can be effectively applied to solve many real-world problems across widely diverse fields. Their success is inseparable from the message-passing mechanisms evolving over the years. However, current mechanisms treat all node features equally at the macro-level (node-level), and the optimal aggregation method has not yet been explored. In this paper, we propose a new GNN called Graph Decipher (GD), which transparentizes the message flows of node features from micro-level (feature-level) to global-level and boosts the performance on node classification tasks. Besides, to reduce the computational burden caused by investigating message-passing, only the relevant representative node attributes are extracted by graph feature filters, allowing calculations to be performed in a category-oriented manner. Experiments on 10 node classification data sets show that GD achieves state-of-the-art performance while imposing a substantially lower computational cost. Additionally, since GD has the ability to explore the representative node attributes by category, it can also be applied to imbalanced node classification on multiclass graph data sets..\",\n   },\n  {\n    author: <div> Vijay Harid, Chao Liu, <span className=\"font-bold\">Yan Pang</span>, Akimun Jannat Alvina, Mark Golkowski, Poorya Hosseini, Morris Cohen</div>,\n    title: \"Automated Large‐Scale Extraction of Whistlers Using Mask‐Scoring Regional Convolutional Neural Network\",\n    jounal: \"Geophysical Research Letters\",\n    paper: \"https://doi.org/10.1029/2021GL093819\",\n    img: \"https://agupubs.onlinelibrary.wiley.com/cms/asset/515d4c0d-3b76-4c45-8e6f-8d3e5639b394/grl62811-fig-0001-m.jpg\",\n    abstract: \"Extremely and very low frequency (ELF/VLF) radio waves are generated from a variety of natural geophysical sources. Ground-based observations often contain signals of interest; however, the signals are typically immersed in a noisy environment due to lightning-generated sferics and additional anthropogenic sources. Although automated detection algorithms have been employed successfully in the past, extraction of arbitrary and broadband signal classes has been a challenge. In this work, we employ a mask-scoring regional convolutional neural network (MSRCNN) for automated extraction of whistlers from ground measurements at Palmer station, Antarctica. Statistics of several hundred thousand whistler receptions are evaluated to determine seasonal and diurnal variations at Palmer station along with strong correlations to lightning activity in the conjugate hemisphere. Although MSRCNN has been employed for whistler extraction in this work, the method has can be easily extended to other signal classes including chorus, hiss, and VLF triggered emissions.\",\n  },\n  {\n    author: <div><span className=\"font-bold\">Yan Pang</span>, Yeyin Shi, Shancheng Gao, Feng Jiang, Arun-Narenthiran Veeranampalayam-Sivakumar, Laura Thompson, Joe Luck, Chao Liu</div>,\n    title: \"Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery\",\n    jounal: \"Computers and Electronics in Agriculture\",\n    paper: \"https://doi.org/10.1016/j.compag.2020.105766\",\n    img: \"https://ars.els-cdn.com/content/image/1-s2.0-S0168169920311376-gr1.jpg\",\n    abstract: \"Stand counts is one of the most common ways farmers assess plant growth conditions and management practices throughout the season. The conventional method for early-season stand count is through manual inspection, which is time-consuming, laborious, and spatially limited in scope. In recent years, Unmanned Aerial Vehicles (UAV) based remote sensing has been widely used in agriculture to provide low-altitude, high spatial resolution imagery to assist decision making. In this project, we designed a system that uses geometric descriptor information with deep neural networks to determine early-season maize stands from relatively low spatial resolution (10 to 25 mm) aerial data, which covers a relatively large area (10 to 25 hectares). Instead of detecting individual crops in a row, we process the entire row at one time, which significantly reduces the requirements for the clarity of the crops. Besides, our new MaxArea Mask Scoring RCNN algorithm could segment crop-rows out in each patch image, regardless of the terrain conditions. The robustness of our scheme was tested on data collected at two different fields in different years. The accuracy of the estimated emergence rate reached up to 95.8%. Due to the high processing speed of the system, it has the potential for real-time applications in the future.\",\n  },\n  {\n    author: <div><span className=\"font-bold\">Yan Pang</span>, Jake Christenson, Feng Jiang, Tim Lei, Remy Rhoades, Drew Kern, John A Thompson, Chao Liu </div>,\n    title: \"Automatic detection and quantification of hand movements toward development of an objective assessment of tremor and bradykinesia in Parkinson's disease\",\n    jounal: \"Journal of neuroscience methods\",\n    paper: \"https://doi.org/10.1016/j.jneumeth.2019.108576\",\n    img: \"https://ars.els-cdn.com/content/image/1-s2.0-S0165027019304339-gr7.jpg\",\n    abstract: \"Classification of parkinsonian symptoms, including tremor and bradykinesia, require the application of validated clinical rating scales which are inherently subjective. In this study, we assessed an objective measure of parkinsonian symptomology using automated analysis of hand gestures. We constructed and evaluated a hand and finger motion capture apparatus and analysis pipeline that recorded hand/finger motion of control subjects and patients with Parkinson's disease. The detailed three-dimensional (3D) motion features of each finger joint was extracted by using Discrete Wavelet Transform (DWT). The severity of tremor for each finger joint was quantitated by analyzing the motion changes in the frequency domain on four types of motion from five patients and twenty-two control subjects. The proposed approach could distinguish the behavior of patients with Parkinson's disease and control subjects by analyzing the detailed motion features of their hands/fingers. Previously established methods to quantitate finger movement dynamics focus on speed and amplitude. In contrast, our approach measures unsupervised motion features, in real-time, using wavelet analysis, of each individual finger joint during active free movement. The proposed study provides an objective assessment of tremor and bradykinesia in Parkinson's disease. Accordingly, this may help movement disorder clinicians to detect, diagnose and monitor treatment efficacy in Parkinson's disease.\",\n  },\n  {\n    author: <div>Feng Jiang, <span className=\"font-bold\">Yan Pang</span>, ThienNgo N Lee, Chao Liu </div>,\n    title: \"Automatic object segmentation based on grabcut\",\n    jounal: \"Science and Information Conference\",\n    paper: \"https://doi.org/10.1007/978-3-030-17795-9_25\",\n    img: \"https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-17795-9_25/MediaObjects/473237_1_En_25_Fig3_HTML.png\",\n    abstract: \"Object segmentation is used in multiple image processing applications. It is generally difficult to perform the object segmentation fully automatically. Most object segmentation schemes are developed based on prior information, training process, existing annotation, special mechanical settings or the human visual system modeling. We proposed a fully automatic segmentation method not relying on any training/learning process, existing annotation, special settings or the human visual system. The automatic object segmentation is accomplished by an objective object weight detection and modified GrabCut segmentation. The segmentation approach we propose is developed only based on the inherent image features. It is independent with various datasets and could be applied to different scenarios. The segmentation result is illustrated by testing a large dataset.\"\n  }\n]\n\n// markup\nconst IndexPage = () => {\n  return (\n    <main style={pageStyles}>\n      <div className=\"bg-black h-12 text-white fixed w-full\">\n        <div className=\"max-w-screen-md max-w-screen-lg mx-auto px-5\">\n          <div className=\"h-12 flex flex-row flex-wrap content-center items-baseline\">\n            <div className=\"mr-5 font-bold\">\n              <a href=\"#top\">Yan Pang</a>\n            </div>\n            <div className=\"mr-5 text-sm text-gray-300 hidden md:block\">\n              <a href=\"#interest\">Research Interests</a>\n            </div>\n            <div className=\"mr-5 text-sm text-gray-300 hidden md:block\">\n              <a href=\"#news\">News</a>\n            </div>\n            <div className=\"mr-5 text-sm text-gray-300 hidden md:block\">\n              <a href=\"#educations\">Educations</a>\n            </div>\n            <div className=\"mr-5 text-sm text-gray-300 hidden md:block\">\n              <a href=\"#publications\">Publications</a>\n            </div>\n            <div className=\"mr-5 text-sm text-gray-300 hidden md:block\">\n              <a href=\"#teaching\">Teaching Experiences</a>\n            </div>\n          </div>\n        </div>\n      </div>\n      <div id=\"top\"></div>\n      <div>\n        <div className=\"max-w-screen-md max-w-screen-lg mx-auto px-5 py-16 md:pt-28\">\n          <div className=\"flex flex-row flex-wrap\">\n            <div>\n              <img className=\"w-full md:w-60\" src=\"https://pbs.twimg.com/media/FH6H1i4UUAEyhAN?format=jpg&name=medium\"></img>\n            </div>\n            <div className=\"flex-1 ml-3 mt-3 md:mt-0\">\n              <p className=\"font-semibold text-sm\" >\n              I am currently an associate professor in the Institute of Artificial Intelligence and Blockchain at Guangzhou University, China. Prior to Guangzhou University, I was fortunate to be a Ph.D student in University of Colorado (2017-2021), advised by Dr. Chao Liu. My research interests span machine learning, computer vision, efficient deep learning, etc.\n              </p>\n              <p className=\"mt-3\">\n              From April 2021 to May 2022, I was a <b> machine learning scientist</b> at Moffett AI, Los Altos, CA. From Aug. 2017 to May 2021, I was an <b>instructor</b> in Department of Electrical Engineering at University of Colorado Denver. From Aug. 2018 to May 2021, I was a <b>instructor</b> in Department of Department of Engineering and Engineering Technology at Metropolitan State University of Denver.\n              </p>\n              <div className=\"mt-3 text-xs text-gray-600\">\n                <div><span className=\"font-bold\">Address:</span> 230 Wai Huan Xi Road, Guangzhou Higher Education Mega Center, Guangzhou 510006 China | <span className=\"font-bold\">Email:</span> yanpangee@gmail.com</div>\n              </div>\n            </div>\n          </div>\n\n          {/* <div className=\"mt-5 text-xs text-gray-600\">\n            <div> 230 Wai Huan Xi Road, Guangzhou Higher Education Mega Center, Guangzhou 510006 P.R.China</div>\n            <div> Email: yanpangee@gmail.com</div>\n          </div> */}\n\n          <div id=\"interests\" className=\"mt-12\">\n            <div className=\"text-blue-800 text-xl font-bold\">Research Interests</div>\n            <p className=\"text-sm\">My research focuses on the broad areas of machine learning, deep learning and their applications on computer vision. Specifically, I focus on </p>\n            <div className=\"text-sm pl-10 py-3\">\n              <ul className=\"list-disc\">\n                <li>Graph Neural Networks</li>\n                <li>Object Detection</li>\n                <li>Key Points Detection</li>\n                <li>Multimodal Learning</li>\n                <li>Knowledge Distillation</li>\n                <li>Photography</li>\n                {/* <li>\n                  <a className=\"text-blue-600 text-sm\" href=\"https://www.instagram.com/pypancho/\" target=\"_blank\">Photography</a>\n                </li> */}\n              </ul>\n            </div>\n          </div>\n\n          <div id=\"news\">\n            <div className=\"text-blue-800 text-xl font-bold\">News</div>\n            <div className=\"text-sm pl-10 py-3\">\n              <ul className=\"list-disc\">\n                <li>[Aug 15, 2022] <b>NEW:</b> Joined to Guangzhou University, Guangdong.</li>\n                <li>[Jul 25, 2022] Two papers published to Journal of International Journal of Intelligent Systems.</li>\n                <li>[Jun 20, 2022] Submitted one paper to Journal of IEEE Transactions on Neural Networks and Learning System.</li>\n                <li>[Dec 18, 2021] Received my Ph.D. degree.</li>\n                <li>[Aug 01, 2021] One paper published to Journal of Geophysical Research Letters. </li>\n                <li>[Apr 12, 2021] Joined to Moffett AI, Los Altos, CA.</li>\n                <li>[Nov 01, 2020] One paper published to Journal of Computers and Electronics in Agriculture. </li>\n                <li>[Mar 01, 2020] One paper published to Journal of neuroscience method. </li>\n                <li>[Apr 24, 2019] One paper accepted to Science and Information Conference</li>\n              </ul>\n            </div>\n          </div>\n\n          <div id=\"educations\">\n            <div className=\"text-blue-800 text-xl font-bold\">Educations</div>\n            <div className=\"text-sm pl-10 py-3\">\n              <ul className=\"list-disc\">\n                <li>2017.08 - 2021.12, University of Colorado, Dept. of Electrical Engineering, Ph.D.</li>\n                <li>2015.08 - 2017.05, University of Wyoming, Dept. of Electrical and Computer Engineering, Ph.D. Student</li>\n                <li>2010.09 - 2013.05, Politecnico di Torino, Dept. of Electrical Engineering, Master</li>\n                <li>2005.09 - 2009.05, Henan Polytechnic University, Dept. of Automation, Bachelor</li>\n              </ul>\n            </div>\n          </div>\n\n          <div id=\"publications\">\n            <div className=\"text-blue-800 text-xl font-bold\">Recent Publications</div>\n            <div className=\"py-3\">\n              {links.map((link, id) =>\n                <div key={id} className=\"mb-8 flex flex-row flex-wrap items-center\">\n                  <div className=\"w-full md:w-60\">\n                    <img className=\"w-full md:w-60\" src={link.img}></img>\n                  </div>\n                  <div className=\"md:ml-5 flex-1\">\n                    <div>{link.author}</div>\n                    <div className=\"font-bold\">{link.title}</div>\n                    <div>{link.jounal}</div>\n                    <div>\n                      <a className=\"text-blue-600 text-sm\" href={link.paper} target=\"_blank\">Paper Link</a>\n                    </div>\n                  </div>\n                  <div className=\"text-sm mt-3 text-gray-500\">\n                    {link.abstract}\n                  </div>\n                </div>\n              )}\n            </div>\n          </div>\n\n          <div id=\"teaching\">\n            <div className=\"text-blue-800 text-xl font-bold\">Teaching Experiences</div>\n            <div className=\"text-sm pl-10 py-3\">\n              <ul className=\"list-disc\">\n                <li>2020.01 - Present, JulyEdu, couses include advanced course of computer vision, graph neural networks, object detection, human pose estimation, object tracking, SLAM, C++, et al.</li>\n                <li>2018.08 - 2021.05, Metropolitan State University of Denver, EET/CPE 2350 Advanced Technical Programming, EET/CPE 3330 Digital Circuits/Systems II，EET/CPE 4020 Digital Circuits/Systems III, CPE 4600 VLSI Circuits and Systems.</li>\n                <li>2017.08 - 2021.05, University of Colorado Denver, ELEC 4561 Hardware and Software Interface, ELEC 2531 Logic Lab.</li>\n              </ul>\n            </div>\n          </div>\n        </div>\n\n        <div className=\"h-48 bg-gray-200 text-center pt-16 text-sm text-gray-500\">\n          <div>© 2022 Yan Pang. All rights reserved</div>\n          <div>(Last update: Aug 15, 2022.)</div>\n        </div>\n      </div>\n    </main>\n  )\n}\n\nexport default IndexPage\n"],"names":["pageStyles","color","fontFamily","links","author","className","title","jounal","paper","img","abstract","style","href","id","src","map","link","key","target"],"sourceRoot":""}