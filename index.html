<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.4d42fdfc2eef5083b696.css" data-identity="gatsby-global-css">/*
! tailwindcss v3.0.8 | MIT License | https://tailwindcss.com
*/*,:after,:before{border:0 solid;box-sizing:border-box}:after,:before{--tw-content:""}html{-webkit-text-size-adjust:100%;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-size:100%;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]{display:none}.fixed{position:fixed}.mx-auto{margin-left:auto;margin-right:auto}.mr-5{margin-right:1.25rem}.ml-3{margin-left:.75rem}.mt-3{margin-top:.75rem}.mt-12{margin-top:3rem}.mb-8{margin-bottom:2rem}.mt-5{margin-top:1.25rem}.flex{display:flex}.hidden{display:none}.h-12{height:3rem}.h-48{height:12rem}.w-full{width:100%}.max-w-screen-md{max-width:768px}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.list-disc{list-style-type:disc}.flex-row{flex-direction:row}.flex-wrap{flex-wrap:wrap}.content-center{align-content:center}.items-center{align-items:center}.items-baseline{align-items:baseline}.bg-black{--tw-bg-opacity:1;background-color:rgb(0 0 0/var(--tw-bg-opacity))}.bg-gray-200{--tw-bg-opacity:1;background-color:rgb(229 231 235/var(--tw-bg-opacity))}.px-5{padding-left:1.25rem;padding-right:1.25rem}.py-16{padding-bottom:4rem;padding-top:4rem}.py-3{padding-bottom:.75rem;padding-top:.75rem}.pl-10{padding-left:2.5rem}.pt-16{padding-top:4rem}.text-center{text-align:center}.text-justify{text-align:justify}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.font-bold{font-weight:700}.font-semibold{font-weight:600}.text-blue-600{--tw-text-opacity:1;color:rgb(37 99 235/var(--tw-text-opacity))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.text-gray-300{--tw-text-opacity:1;color:rgb(209 213 219/var(--tw-text-opacity))}.text-gray-600{--tw-text-opacity:1;color:rgb(75 85 99/var(--tw-text-opacity))}.text-blue-800{--tw-text-opacity:1;color:rgb(30 64 175/var(--tw-text-opacity))}.text-gray-500{--tw-text-opacity:1;color:rgb(107 114 128/var(--tw-text-opacity))}@media (min-width:768px){.md\:mt-0{margin-top:0}.md\:ml-5{margin-left:1.25rem}.md\:block{display:block}.md\:w-60{width:15rem}.md\:pt-28{padding-top:7rem}}</style><meta name="generator" content="Gatsby 4.4.0"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><title data-react-helmet="true"></title><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="icon" href="/favicon-32x32.png?v=53aa06cf17e4239d0dba6ffd09854e02" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link as="script" rel="preload" href="/webpack-runtime-950de90d59b9fc614603.js"/><link as="script" rel="preload" href="/framework-5aba6dc98ac29269aae0.js"/><link as="script" rel="preload" href="/app-f4756d18539972e693c4.js"/><link as="script" rel="preload" href="/component---src-pages-index-js-60e98720b7a5684b1f11.js"/><link as="fetch" rel="preload" href="/page-data/index/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><main style="color:#232129;font-family:Georgia, Times New Roman, Times, serif"><div class="bg-black h-12 text-white fixed w-full"><div class="max-w-screen-md max-w-screen-lg mx-auto px-5"><div class="h-12 flex flex-row flex-wrap content-center items-baseline"><div class="mr-5 font-bold"><a href="#top">Yan Pang</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#interest">Research Interests</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#news">News</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#educations">Educations</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#publications">Publications</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#presentations"> Invited Talks</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#patents">Patents</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#teaching">Teaching Experiences</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#students">Students</a></div></div></div></div><div id="top"></div><div><div class="max-w-screen-md max-w-screen-lg mx-auto px-5 py-16 md:pt-28"><div class="flex flex-row flex-wrap"><div><img class="w-full md:w-60" src="https://pbs.twimg.com/media/FH6H1i4UUAEyhAN?format=jpg&amp;name=medium"/></div><div class="flex-1 ml-3 mt-3 md:mt-0 text-justify"><p class="mt-3"><b>Associate professor</b> at the <b>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</b>, China. </p><p class="mt-3">Dr. Pang earned his Ph.D. from the University of Colorado in 2021. His research focuses on computer vision, smart healthcare, and embodied intelligent surgical robot navigation systems. Between September 2022 and September 2025, he served as an associate professor at Guangzhou University in China. Prior to this, Dr. Pang worked as a machine learning scientist at Moffett AI in Los Altos, CA, from April 2021 to May 2022. He also held instructor positions at the University of Colorado Denver (Department of Electrical Engineering) and Metropolitan State University of Denver (Department of Electrical Engineering Technology) from August 2018 to May 2021.</p><div class="mt-3 text-xs text-gray-600"><div> <span class="font-bold">Address:</span> 1068 Xueyuan Avenue, Shenzhen University Town, Shenzhen, 518055 China | <span class="font-bold">Email:</span> yanpang@siat.ac.cn</div></div></div></div><div id="interests" class="mt-12"><div class="text-blue-800 text-xl font-bold">Research Interests</div><p class="text-sm">My research focuses on the broad areas of machine learning, deep learning and their applications on computer vision. Specifically, I focus on </p><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>Medical Image Analysis</li><li>Embodied Intelligent Surgical Robot</li><li>Large-scale Vision Models</li><li>On-device Models</li><li>Graph Neural Networks</li><li>Semantic Segmentation</li><li>Key Points Detection</li><li>Blockchain Security</li><li>Photography</li></ul></div></div><div id="news"><div class="text-blue-800 text-xl font-bold">News</div><div class="text-sm pl-10 py-3 text-justify"><ul class="list-disc"><li>[Sep 29, 2025] <b>NEW:</b> One paper has been published in the IEEE Transactions on Medical Imaging. (IF: 9.8).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/EAT" target="_blank">Code</a> is released.</li><li>[Sep 05, 2025] <b>NEW:</b> One paper has been published in the IEEE Journal of Biomedical and Health Informatics. (IF: 7.7).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/SegTom" target="_blank">Code</a> is released.</li><li>[Aug 25, 2025] One paper has been published in the IEEE Transactions on Medical Imaging. (IF: 9.8).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/Slim-UNETRV2" target="_blank">Code</a> is released.</li><li>[Aug 23, 2025] 🎉 Success! Papers 1614, 1833, and 2000 from our group are presented at <a class="text-blue-600 text-sm" href="http://2025.prcv.cn/" target="_blank">PRCV 2025</a> in Shanghai. Proud of my students&#x27; contributions to the field! </li><li>[Jul 30, 2025] One paper has been published in the IEEE Transactions on Big Data. (IF: 5.7).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/Consense" target="_blank">Code</a> is released.</li><li>[Jul 14, 2025] One paper has been published in the IEEE Transactions on Information Forensics and Security. (IF: 8).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/SAMamba" target="_blank">Code</a> is released.</li><li>[Jun 17, 2025] One paper has been published in the IEEE Transactions on Systems, Man and Cybernetics: Systems. (IF: 8.7).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/LGA" target="_blank">Code</a> is released.</li><li>[Feb 19, 2025] One paper has been published in the IEEE Journal of Biomedical and Health Informatics. (IF: 7.7).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/BaS" target="_blank">Code</a> is released.</li><li>[Jan 16, 2025] One paper has been published in the IEEE Journal of Biomedical and Health Informatics. (IF: 7.7).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/MOD" target="_blank">Code</a> is released.</li><li>[Dec 12, 2024] <b>NEW:</b> Welcome to submit your papers to our special topics &quot;<a class="text-blue-600 text-sm" href="https://www.mdpi.com/topics/0ZZS608U21" target="_blank">AI and Data-Driven Advancements in Industry 4.0, 2nd Edition</a>&quot;.</li><li>[Oct 19, 2024] Honored to be named one of the best ACs for PRCV2024! Excited to contribute to this leading conference in computer vision.</li><li>[Oct 18, 2024] 🎉 Success! Papers 1458, 1464, and 1468 from our group are presented at <a class="text-blue-600 text-sm" href="http://2024.prcv.cn/" target="_blank">PRCV 2024</a> in Urumqi. Proud of my students&#x27; contributions to the field! </li><li>[Apr 21, 2024] One paper has been published in the Journal of IEEE Transactions on Instrumentation and Measurement. (IF: 5.6).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/AdaptFormer" target="_blank">Code</a> is released.</li><li>[Oct 20, 2023] One paper has been published in the Journal of IEEE Transactions on Medical Imaging. (IF: 10.6).  <a class="text-blue-600 text-sm" href="https://github.com/deepang-ai/Slim-UNETR" target="_blank">Code</a> is released.</li><li>[Aug 14, 2023] One paper has been published in the Journal of IEEE Transactions on Neural Networks and Learning Systems. (IF: 10.4).</li><li>[Feb 07, 2023] One paper published in the Journal of Information Sciences (IF: 8.233).</li><li>[Jan 06, 2023] Become a topic editor of the &quot;<a class="text-blue-600 text-sm" href="https://www.mdpi.com/topics/FLN51J9SH1" target="_blank">AI and Data-Driven Advancements in Industry 4.0</a>&quot;. Welcome to submit your papers on AI.</li><li>[Aug 15, 2022] Joined to Guangzhou University, Guangdong.</li><li>[Jul 25, 2022] Two papers published in the International Journal of Intelligent Systems （IF：8.993）.</li><li>[Dec 18, 2021] Received my Ph.D. degree.</li><li>[Aug 01, 2021] One paper published in the Journal of Geophysical Research Letters (IF: 5.58). </li><li>[Apr 12, 2021] Joined to Moffett AI, Los Altos, CA.</li><li>[Nov 01, 2020] One paper has been published in the Journal of Computers and Electronics in Agriculture. </li><li>[Mar 01, 2020] One paper has been published in the Journal of Neuroscience Methods. </li><li>[Apr 24, 2019] One paper is accepted to Science and Information Conference</li></ul></div></div><div id="educations"><div class="text-blue-800 text-xl font-bold">Educations</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>2017.08 - 2021.12, University of Colorado, Dept. of Electrical Engineering, Ph.D.</li><li>2015.08 - 2017.05, University of Wyoming, Dept. of Electrical and Computer Engineering, Ph.D. Student</li><li>2010.09 - 2013.05, Politecnico di Torino, Dept. of Electrical Engineering, Master</li><li>2005.09 - 2009.05, Henan Polytechnic University, Dept. of Automation, Bachelor</li></ul></div></div><div id="publications"><div class="text-blue-800 text-xl font-bold">Journal Publications</div><div class="py-3"><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Yucheng Long, Zibin Chen, Ying Hu, Hao Chen, Qiong Wang</div></div><div class="font-bold">Endoscopic Adaptive Transformer for Enhanced Polyp Segmentation in Endoscopic Imaging</div><div>IEEE Transactions on Medical Imaging</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/TMI.2025.3615677" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Yunhao Li, Jiaming Liang, Hao Chen, Ying Hu, Qiong Wang</div></div><div class="font-bold">SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</div><div>IEEE Journal of Biomedical and Health Informatics</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/JBHI.2025.3606266" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Jiaming Liang, Junming Yan, Ying Hu, Hao Chen, Qiong Wang</div></div><div class="font-bold">Slim UNETRV2: 3D Image Segmentation for Resource-Limited Medical Portable Devices</div><div>IEEE Transactions on Medical Imaging</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/TMI.2025.3602145" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> <span class="font-bold">Yan Pang</span>, Xiangfu Liu, Teng Huang, Yile Hong, Jiahui Huang, Changyu Dong</div></div><div class="font-bold">Graph-based Contract Sensing Framework for Smart Contract Vulnerability Detection</div><div>IEEE Transactions on Big Data</div><div><a class="text-blue-600 text-sm" href="https://ieeexplore.ieee.org/document/11104930" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> Teng Huang, Jiahui Huang, Changyu Dong, Sisi Duan, <span class="font-bold">Yan Pang *</span></div></div><div class="font-bold">SAMamba: Structure-Aware Mamba for Ethereum Fraud Detection</div><div>IEEE Transactions on Information Forensics and Security</div><div><a class="text-blue-600 text-sm" href="https://ieeexplore.ieee.org/document/11080015" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> Jiahui Huang, Teng Huang, Changyu Dong, Sisi Duan, <span class="font-bold">Yan Pang *</span></div></div><div class="font-bold">Hierarchical Network with Local-Global Awareness for Ethereum Account De-anonymization</div><div>IEEE Transactions on Systems, Man and Cybernetics: Systems</div><div><a class="text-blue-600 text-sm" href="https://ieeexplore.ieee.org/document/11037616" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> <span class="font-bold">Yan Pang </span>, Yunhao Li, Teng Huang, Jiaming Liang, Ziyu Ding, Hao Chen, Baoliang Zhao, Ying Hu, Zheng Zhang, Qiong Wang</div></div><div class="font-bold">Efficient Breast Lesion Segmentation from Ultrasound Videos Across Multiple Source-limited Platforms</div><div>IEEE Journal of Biomedical and Health Informatics</div><div><a class="text-blue-600 text-sm" href="https://ieeexplore.ieee.org/document/10892059" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> <span class="font-bold">Yan Pang </span>, Yunhao Li, Teng Huang, Jiaming Liang, Zhen Wang, Changyu Dong, Dongyang Kuang, Ying Hu, Hao Chen, Tim Lei, Qiong Wang</div></div><div class="font-bold">Online Self-distillation and Self-modeling for 3D Brain Tumor Segmentation</div><div>IEEE Journal of Biomedical and Health Informatics</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/JBHI.2025.3530715" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div>Teng Huang, Yile Hong, <span class="font-bold">Yan Pang *</span>, Jiaming Liang, Jie Hong, Lin Huang, Yuan Zhang, Yan Jia, Patrizia Savi</div></div><div class="font-bold">AdaptFormer: An Adaptive Hierarchical Semantic Approach for Change Detection on Remote Sensing Images</div><div>IEEE Transactions on Instrumentation and Measurement</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/TIM.2024.3387494" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Jiaming Liang, Teng Huang, Hao Chen, Yunhao Li, Dan Li, Lin Huang, Qiong Wang</div></div><div class="font-bold">Slim UNETR: Scale Hybrid Transformers to Efficient 3D Medical Image Segmentation Under Limited Computational Resources</div><div>IEEE Transactions on Medical Imaging</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/TMI.2023.3326188" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div>Zhen Wang, Yang Zhang, <span class="font-bold">Yan Pang *</span>, Nannan Wang, Mohamed Jaward Bah, Ke Li, Ji Zhang</div></div><div class="font-bold">Toward Learning Joint Inference Tasks for IASS-MTS Using Dual Attention Memory With Stochastic Generative Imputation</div><div>IEEE Transactions on Neural Networks and Learning Systems</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1109/TNNLS.2023.3305542" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div>Teng Huang, Jiahui Huang, <span class="font-bold">Yan Pang *</span>, Hongyang Yan</div></div><div class="font-bold">Smart Contract Watermarking Based on Code Obfuscation</div><div>Information Sciences</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1016/j.ins.2023.01.126" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Ai Shan, Zhen Wang, Mengyu Wang, Jianwei Li, Ji Zhang, Teng Huang, Chao Liu </div></div><div class="font-bold">Sparse-Dyn: Sparse dynamic graph multirepresentation learning via event-based sparse temporal attention network</div><div>International Journal of Intelligent Systems</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1002/int.22967" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Teng Huang, Zhen Wang, Jianwei Li, Poorya Hosseini, Ji Zhang, Chao Liu, Shan Ai </div></div><div class="font-bold">Graph Decipher: A transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification</div><div>International Journal of Intelligent Systems</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1002/int.22966" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div> Vijay Harid, Chao Liu, <span class="font-bold">Yan Pang</span>, Akimun Jannat Alvina, Mark Golkowski, Poorya Hosseini, Morris Cohen</div></div><div class="font-bold">Automated Large‐Scale Extraction of Whistlers Using Mask‐Scoring Regional Convolutional Neural Network</div><div>Geophysical Research Letters</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1029/2021GL093819" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Yeyin Shi, Shancheng Gao, Feng Jiang, Arun-Narenthiran Veeranampalayam-Sivakumar, Laura Thompson, Joe Luck, Chao Liu</div></div><div class="font-bold">Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery</div><div>Computers and Electronics in Agriculture</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1016/j.compag.2020.105766" target="_blank">Paper Link</a></div></div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Jake Christenson, Feng Jiang, Tim Lei, Remy Rhoades, Drew Kern, John A Thompson, Chao Liu </div></div><div class="font-bold">Automatic detection and quantification of hand movements toward development of an objective assessment of tremor and bradykinesia in Parkinson&#x27;s disease</div><div>Journal of neuroscience methods</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1016/j.jneumeth.2019.108576" target="_blank">Paper Link</a></div></div></div></div></div><div id="presentations"><div class="text-blue-800 text-xl font-bold">Invited Talks</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>[Sep 19th, 2025] Jinan, China:<div>“Real-Time Medical Image Processing via On-Device Model Architectures Integrated on AI Chips”, China Association of Chinese Medicine;</div></li><li>[Jul 20th, 2025] Shenzhen, China:<div>“Embodied Intelligent Surgical Robots: A Paradigm Shift from Passive Tools to Active Partners”, South China Hospital of Shenzhen University;</div></li><li>[Feb 15th, 2025] Guangzhou, China:<div>“Enabling Real-Time Medical Imaging on Edge Devices: Co-Optimizing Accuracy and Computational Efficiency”, 17th International Conference on Machine Learning and Computing 2025;</div></li><li>[May 25th, 2024] Haikou, China:<div>“Optimizing Medical Image Analysis: Boosting Model Performance with Integrated Software and Hardware Solutions”, International Conference on Digital Image Processing 2024;</div></li><li>[Apr 14, 2024] Hangzhou, China:<div>“Improving Medical Image Analysis by Balancing Accuracy and Efficiency on Resource-Constrained Platforms”, The 3rd National Conference on Electronic Information Materials and Devices;</div></li><li>[Feb. 5th, 2024] Shenzhen, China:<div>“Enhancing Medical Image Analysis by Achieving an Optimal Balance Between Accuracy and Efficiency in Resource-Limited Platforms”, 16th International Conference on Machine Learning and Computing;</div></li><li>[Dec. 22nd, 2023] Shanghai, China:<div>“Adaptive Hierarchical Semantic Approach for Change Detection”, The 9th International Conference on Signal Processing;</div></li><li>[Dec. 12th, 2023] Macao, China:<div>“Optimizing Machine Learning Algorithms for Coordinated Software and Hardware Performance”, University of Macao;</div></li><li>[Dec. 1st, 2023] Luoyang, China:<div>“Application of Large-scale Model of Multimodal Medical Image Analysis”, The First Affiliated Hospital of Henan University of Science &amp; Technology;</div></li><li>[Jun. 7th, 2023] Zhuhai, China:<div>“Adaptive Graph Representation Learning”, Sun Yat-Sen University;</div></li></ul></div></div><div id="patents"><div class="text-blue-800 text-xl font-bold">Patents</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>一种文本生成粤剧视频的方法、装置、电子设备及介质，授权号: ZL202411065751.1</li><li>基于多模态图像的脊柱检测方法、装置、设备及存储介质，授权号: ZL202411752951.4</li><li>一种医学图像分割方法、装置及介质，授权号: ZL202410616680.3</li><li>用户身份识别方法、系统、设备及存储介质, 授权号: ZL202410633064.9</li><li>防御模型被窃取攻击的方法、系统、设备及存储介质, 授权号: ZL202410456738.2 </li><li>自蒸馏与自学习的医学图像分割方法、装置及存储介质, 授权号: ZL202410615709.6 </li><li>医学视频乳腺病变特征快速分割方法、装置、设备及介质, 授权号: ZL202410485386.3 </li><li>基于标记移除网络的高精度钢琴手势数据集构建方法, 授权号: ZL202411072622.5 </li><li>图像识别方法、装置、设备及存储介质, 授权号: ZL202410623942.9 </li><li>智能合约的漏洞检测方法、装置、设备及介质, 授权号: ZL202410547779.2 </li></ul></div></div><div id="teaching"><div class="text-blue-800 text-xl font-bold">Teaching Experiences</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>2022.09 - 2025.09 Guangzhou University:<div>Computer Vision Processing(181940016-1), Natural Language Processing(180600065-1)，Assembly Language Processing(210600058-1).</div></li><li>2018.08 - 2021.05, Metropolitan State University of Denver:<div>EET/CPE 2350 Advanced Technical Programming, EET/CPE 3330 Digital Circuits/Systems II, EET/CPE 4020 Digital Circuits/Systems III, CPE 4600 VLSI Circuits and Systems.</div></li><li>2017.08 - 2021.05, University of Colorado Denver:<div>ELEC 4561 Hardware and Software Interface, ELEC 2531 Logic Lab.</div></li></ul></div></div><div id="students"><div class="text-blue-800 text-xl font-bold">Students</div><p class="text-sm font-bold">  Masters: </p><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>Graduates of 2024: Jie Hong, Dan Li, <a class="text-blue-600 text-sm" href="https://jeming-creater.github.io/" target="_blank">Jiaming Liang</a></li><li>Graduates of 2025: <a class="text-blue-600 text-sm" href="https://yunhaoli.top/" target="_blank">Yunhao Li</a>, Yile Hong, Hui Li, Mingwei Chen</li><li>Third-Year Student: Caiyan Tan, Yanjun Ming, Mingdu Zhang, Xiangfu Liu, Zibin Chen</li><li>Second-Year Student: Aoying Wang, Xing Wang</li></ul></div></div></div><div class="h-48 bg-gray-200 text-center pt-16 text-sm text-gray-500"><div>© 2025 Yan Pang. All rights reserved</div><div>(Last update: Oct 01, 2025.)</div></div></div></main></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-6a71d9fc662cbda582e6.js"],"app":["/app-f4756d18539972e693c4.js"],"component---src-pages-404-js":["/component---src-pages-404-js-00e77064645ae1056ad6.js"],"component---src-pages-index-js":["/component---src-pages-index-js-60e98720b7a5684b1f11.js"],"component---src-pages-index-old-js":["/component---src-pages-index-old-js-550796386d2c7103a1d8.js"]};/*]]>*/</script><script src="/polyfill-6a71d9fc662cbda582e6.js" nomodule=""></script><script src="/component---src-pages-index-js-60e98720b7a5684b1f11.js" async=""></script><script src="/app-f4756d18539972e693c4.js" async=""></script><script src="/framework-5aba6dc98ac29269aae0.js" async=""></script><script src="/webpack-runtime-950de90d59b9fc614603.js" async=""></script></body></html>