<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.4d42fdfc2eef5083b696.css" data-identity="gatsby-global-css">/*
! tailwindcss v3.0.8 | MIT License | https://tailwindcss.com
*/*,:after,:before{border:0 solid;box-sizing:border-box}:after,:before{--tw-content:""}html{-webkit-text-size-adjust:100%;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-size:100%;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]{display:none}.fixed{position:fixed}.mx-auto{margin-left:auto;margin-right:auto}.mr-5{margin-right:1.25rem}.ml-3{margin-left:.75rem}.mt-3{margin-top:.75rem}.mt-12{margin-top:3rem}.mb-8{margin-bottom:2rem}.mt-5{margin-top:1.25rem}.flex{display:flex}.hidden{display:none}.h-12{height:3rem}.h-48{height:12rem}.w-full{width:100%}.max-w-screen-md{max-width:768px}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.list-disc{list-style-type:disc}.flex-row{flex-direction:row}.flex-wrap{flex-wrap:wrap}.content-center{align-content:center}.items-center{align-items:center}.items-baseline{align-items:baseline}.bg-black{--tw-bg-opacity:1;background-color:rgb(0 0 0/var(--tw-bg-opacity))}.bg-gray-200{--tw-bg-opacity:1;background-color:rgb(229 231 235/var(--tw-bg-opacity))}.px-5{padding-left:1.25rem;padding-right:1.25rem}.py-16{padding-bottom:4rem;padding-top:4rem}.py-3{padding-bottom:.75rem;padding-top:.75rem}.pl-10{padding-left:2.5rem}.pt-16{padding-top:4rem}.text-center{text-align:center}.text-justify{text-align:justify}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.font-bold{font-weight:700}.font-semibold{font-weight:600}.text-blue-600{--tw-text-opacity:1;color:rgb(37 99 235/var(--tw-text-opacity))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.text-gray-300{--tw-text-opacity:1;color:rgb(209 213 219/var(--tw-text-opacity))}.text-gray-600{--tw-text-opacity:1;color:rgb(75 85 99/var(--tw-text-opacity))}.text-blue-800{--tw-text-opacity:1;color:rgb(30 64 175/var(--tw-text-opacity))}.text-gray-500{--tw-text-opacity:1;color:rgb(107 114 128/var(--tw-text-opacity))}@media (min-width:768px){.md\:mt-0{margin-top:0}.md\:ml-5{margin-left:1.25rem}.md\:block{display:block}.md\:w-60{width:15rem}.md\:pt-28{padding-top:7rem}}</style><meta name="generator" content="Gatsby 4.4.0"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><title data-react-helmet="true"></title><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="icon" href="/favicon-32x32.png?v=53aa06cf17e4239d0dba6ffd09854e02" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=53aa06cf17e4239d0dba6ffd09854e02"/><link as="script" rel="preload" href="/webpack-runtime-eeb24cc78a6f6e7e0a6a.js"/><link as="script" rel="preload" href="/framework-5aba6dc98ac29269aae0.js"/><link as="script" rel="preload" href="/app-f4756d18539972e693c4.js"/><link as="script" rel="preload" href="/component---src-pages-index-old-js-550796386d2c7103a1d8.js"/><link as="fetch" rel="preload" href="/page-data/index_old/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><main style="color:#232129;font-family:Georgia, Times New Roman, Times, serif"><div class="bg-black h-12 text-white fixed w-full"><div class="max-w-screen-md max-w-screen-lg mx-auto px-5"><div class="h-12 flex flex-row flex-wrap content-center items-baseline"><div class="mr-5 font-bold"><a href="#top">Yan Pang</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#interest">Research Interests</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#news">News</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#educations">Educations</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#publications">Publications</a></div><div class="mr-5 text-sm text-gray-300 hidden md:block"><a href="#teaching">Teaching Experiences</a></div></div></div></div><div id="top"></div><div><div class="max-w-screen-md max-w-screen-lg mx-auto px-5 py-16 md:pt-28"><div class="flex flex-row flex-wrap"><div><img class="w-full md:w-60" src="https://pbs.twimg.com/media/FH6H1i4UUAEyhAN?format=jpg&amp;name=medium"/></div><div class="flex-1 ml-3 mt-3 md:mt-0"><p class="font-semibold text-sm">I am currently an associate professor in the Institute of Artificial Intelligence and Blockchain at Guangzhou University, China. Prior to Guangzhou University, I was fortunate to be a Ph.D student in University of Colorado (2017-2021), advised by Dr. Chao Liu. My research interests span machine learning, computer vision, efficient deep learning, etc.</p><p class="mt-3">From April 2021 to May 2022, I was a <b> machine learning scientist</b> at Moffett AI, Los Altos, CA. From Aug. 2017 to May 2021, I was an <b>instructor</b> in Department of Electrical Engineering at University of Colorado Denver. From Aug. 2018 to May 2021, I was a <b>instructor</b> in Department of Department of Engineering and Engineering Technology at Metropolitan State University of Denver.</p><div class="mt-3 text-xs text-gray-600"><div><span class="font-bold">Address:</span> 230 Wai Huan Xi Road, Guangzhou Higher Education Mega Center, Guangzhou 510006 China | <span class="font-bold">Email:</span> yanpangee@gmail.com</div></div></div></div><div id="interests" class="mt-12"><div class="text-blue-800 text-xl font-bold">Research Interests</div><p class="text-sm">My research focuses on the broad areas of machine learning, deep learning and their applications on computer vision. Specifically, I focus on </p><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>Graph Neural Networks</li><li>Object Detection</li><li>Key Points Detection</li><li>Multimodal Learning</li><li>Knowledge Distillation</li><li>Photography</li></ul></div></div><div id="news"><div class="text-blue-800 text-xl font-bold">News</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>[Aug 15, 2022] <b>NEW:</b> Joined to Guangzhou University, Guangdong.</li><li>[Jul 25, 2022] Two papers published to Journal of International Journal of Intelligent Systems.</li><li>[Jun 20, 2022] Submitted one paper to Journal of IEEE Transactions on Neural Networks and Learning System.</li><li>[Dec 18, 2021] Received my Ph.D. degree.</li><li>[Aug 01, 2021] One paper published to Journal of Geophysical Research Letters. </li><li>[Apr 12, 2021] Joined to Moffett AI, Los Altos, CA.</li><li>[Nov 01, 2020] One paper published to Journal of Computers and Electronics in Agriculture. </li><li>[Mar 01, 2020] One paper published to Journal of neuroscience method. </li><li>[Apr 24, 2019] One paper accepted to Science and Information Conference</li></ul></div></div><div id="educations"><div class="text-blue-800 text-xl font-bold">Educations</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>2017.08 - 2021.12, University of Colorado, Dept. of Electrical Engineering, Ph.D.</li><li>2015.08 - 2017.05, University of Wyoming, Dept. of Electrical and Computer Engineering, Ph.D. Student</li><li>2010.09 - 2013.05, Politecnico di Torino, Dept. of Electrical Engineering, Master</li><li>2005.09 - 2009.05, Henan Polytechnic University, Dept. of Automation, Bachelor</li></ul></div></div><div id="publications"><div class="text-blue-800 text-xl font-bold">Recent Publications</div><div class="py-3"><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://onlinelibrary.wiley.com/cms/asset/00a3448e-89d1-4e24-9bc8-c3075a20b7e5/int22967-fig-0001-m.jpg"/></div><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Ai Shan, Zhen Wang, Mengyu Wang, Jianwei Li, Ji Zhang, Teng Huang, Chao Liu </div></div><div class="font-bold">Sparse-Dyn: Sparse dynamic graph multirepresentation learning via event-based sparse temporal attention network</div><div>International Journal of Intelligent Systems</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1002/int.22967" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Dynamic graph neural networks (DGNNs) have been widely used in modeling and representation learning of graph structure data. Current dynamic representation learning focuses on either discrete learning which results in temporal information loss, or continuous learning which involves heavy computation. In this study, we proposed a novel DGNN, sparse dynamic (Sparse-Dyn). It adaptively encodes temporal information into a sequence of patches with an equal amount of temporal-topological structure. Therefore, while avoiding using snapshots which cause information loss, it also achieves a finer time granularity, which is close to what continuous networks could provide. In addition, we also designed a lightweight module, Sparse Temporal Transformer, to compute node representations through structural neighborhoods and temporal dynamics. Since the fully connected attention conjunction is simplified, the computation cost is far lower than the current state-of-the-art. Link prediction experiments are conducted on both continuous and discrete graph data sets. By comparing several state-of-the-art graph embedding baselines, the experimental results demonstrate that Sparse-Dyn has a faster inference speed while having competitive performance.</div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://onlinelibrary.wiley.com/cms/asset/18dc386e-138f-4293-a5e9-aa30d31459f4/int22966-fig-0001-m.jpg"/></div><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Teng Huang, Zhen Wang, Jianwei Li, Poorya Hosseini, Ji Zhang, Chao Liu, Shan Ai </div></div><div class="font-bold">Graph Decipher: A transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification</div><div>International Journal of Intelligent Systems</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1002/int.22966" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Graph neural networks (GNNs) can be effectively applied to solve many real-world problems across widely diverse fields. Their success is inseparable from the message-passing mechanisms evolving over the years. However, current mechanisms treat all node features equally at the macro-level (node-level), and the optimal aggregation method has not yet been explored. In this paper, we propose a new GNN called Graph Decipher (GD), which transparentizes the message flows of node features from micro-level (feature-level) to global-level and boosts the performance on node classification tasks. Besides, to reduce the computational burden caused by investigating message-passing, only the relevant representative node attributes are extracted by graph feature filters, allowing calculations to be performed in a category-oriented manner. Experiments on 10 node classification data sets show that GD achieves state-of-the-art performance while imposing a substantially lower computational cost. Additionally, since GD has the ability to explore the representative node attributes by category, it can also be applied to imbalanced node classification on multiclass graph data sets..</div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://agupubs.onlinelibrary.wiley.com/cms/asset/515d4c0d-3b76-4c45-8e6f-8d3e5639b394/grl62811-fig-0001-m.jpg"/></div><div class="md:ml-5 flex-1"><div><div> Vijay Harid, Chao Liu, <span class="font-bold">Yan Pang</span>, Akimun Jannat Alvina, Mark Golkowski, Poorya Hosseini, Morris Cohen</div></div><div class="font-bold">Automated Large‐Scale Extraction of Whistlers Using Mask‐Scoring Regional Convolutional Neural Network</div><div>Geophysical Research Letters</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1029/2021GL093819" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Extremely and very low frequency (ELF/VLF) radio waves are generated from a variety of natural geophysical sources. Ground-based observations often contain signals of interest; however, the signals are typically immersed in a noisy environment due to lightning-generated sferics and additional anthropogenic sources. Although automated detection algorithms have been employed successfully in the past, extraction of arbitrary and broadband signal classes has been a challenge. In this work, we employ a mask-scoring regional convolutional neural network (MSRCNN) for automated extraction of whistlers from ground measurements at Palmer station, Antarctica. Statistics of several hundred thousand whistler receptions are evaluated to determine seasonal and diurnal variations at Palmer station along with strong correlations to lightning activity in the conjugate hemisphere. Although MSRCNN has been employed for whistler extraction in this work, the method has can be easily extended to other signal classes including chorus, hiss, and VLF triggered emissions.</div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://ars.els-cdn.com/content/image/1-s2.0-S0168169920311376-gr1.jpg"/></div><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Yeyin Shi, Shancheng Gao, Feng Jiang, Arun-Narenthiran Veeranampalayam-Sivakumar, Laura Thompson, Joe Luck, Chao Liu</div></div><div class="font-bold">Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery</div><div>Computers and Electronics in Agriculture</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1016/j.compag.2020.105766" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Stand counts is one of the most common ways farmers assess plant growth conditions and management practices throughout the season. The conventional method for early-season stand count is through manual inspection, which is time-consuming, laborious, and spatially limited in scope. In recent years, Unmanned Aerial Vehicles (UAV) based remote sensing has been widely used in agriculture to provide low-altitude, high spatial resolution imagery to assist decision making. In this project, we designed a system that uses geometric descriptor information with deep neural networks to determine early-season maize stands from relatively low spatial resolution (10 to 25 mm) aerial data, which covers a relatively large area (10 to 25 hectares). Instead of detecting individual crops in a row, we process the entire row at one time, which significantly reduces the requirements for the clarity of the crops. Besides, our new MaxArea Mask Scoring RCNN algorithm could segment crop-rows out in each patch image, regardless of the terrain conditions. The robustness of our scheme was tested on data collected at two different fields in different years. The accuracy of the estimated emergence rate reached up to 95.8%. Due to the high processing speed of the system, it has the potential for real-time applications in the future.</div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://ars.els-cdn.com/content/image/1-s2.0-S0165027019304339-gr7.jpg"/></div><div class="md:ml-5 flex-1"><div><div><span class="font-bold">Yan Pang</span>, Jake Christenson, Feng Jiang, Tim Lei, Remy Rhoades, Drew Kern, John A Thompson, Chao Liu </div></div><div class="font-bold">Automatic detection and quantification of hand movements toward development of an objective assessment of tremor and bradykinesia in Parkinson&#x27;s disease</div><div>Journal of neuroscience methods</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1016/j.jneumeth.2019.108576" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Classification of parkinsonian symptoms, including tremor and bradykinesia, require the application of validated clinical rating scales which are inherently subjective. In this study, we assessed an objective measure of parkinsonian symptomology using automated analysis of hand gestures. We constructed and evaluated a hand and finger motion capture apparatus and analysis pipeline that recorded hand/finger motion of control subjects and patients with Parkinson&#x27;s disease. The detailed three-dimensional (3D) motion features of each finger joint was extracted by using Discrete Wavelet Transform (DWT). The severity of tremor for each finger joint was quantitated by analyzing the motion changes in the frequency domain on four types of motion from five patients and twenty-two control subjects. The proposed approach could distinguish the behavior of patients with Parkinson&#x27;s disease and control subjects by analyzing the detailed motion features of their hands/fingers. Previously established methods to quantitate finger movement dynamics focus on speed and amplitude. In contrast, our approach measures unsupervised motion features, in real-time, using wavelet analysis, of each individual finger joint during active free movement. The proposed study provides an objective assessment of tremor and bradykinesia in Parkinson&#x27;s disease. Accordingly, this may help movement disorder clinicians to detect, diagnose and monitor treatment efficacy in Parkinson&#x27;s disease.</div></div><div class="mb-8 flex flex-row flex-wrap items-center"><div class="w-full md:w-60"><img class="w-full md:w-60" src="https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-17795-9_25/MediaObjects/473237_1_En_25_Fig3_HTML.png"/></div><div class="md:ml-5 flex-1"><div><div>Feng Jiang, <span class="font-bold">Yan Pang</span>, ThienNgo N Lee, Chao Liu </div></div><div class="font-bold">Automatic object segmentation based on grabcut</div><div>Science and Information Conference</div><div><a class="text-blue-600 text-sm" href="https://doi.org/10.1007/978-3-030-17795-9_25" target="_blank">Paper Link</a></div></div><div class="text-sm mt-3 text-gray-500">Object segmentation is used in multiple image processing applications. It is generally difficult to perform the object segmentation fully automatically. Most object segmentation schemes are developed based on prior information, training process, existing annotation, special mechanical settings or the human visual system modeling. We proposed a fully automatic segmentation method not relying on any training/learning process, existing annotation, special settings or the human visual system. The automatic object segmentation is accomplished by an objective object weight detection and modified GrabCut segmentation. The segmentation approach we propose is developed only based on the inherent image features. It is independent with various datasets and could be applied to different scenarios. The segmentation result is illustrated by testing a large dataset.</div></div></div></div><div id="teaching"><div class="text-blue-800 text-xl font-bold">Teaching Experiences</div><div class="text-sm pl-10 py-3"><ul class="list-disc"><li>2020.01 - Present, JulyEdu, couses include advanced course of computer vision, graph neural networks, object detection, human pose estimation, object tracking, SLAM, C++, et al.</li><li>2018.08 - 2021.05, Metropolitan State University of Denver, EET/CPE 2350 Advanced Technical Programming, EET/CPE 3330 Digital Circuits/Systems II，EET/CPE 4020 Digital Circuits/Systems III, CPE 4600 VLSI Circuits and Systems.</li><li>2017.08 - 2021.05, University of Colorado Denver, ELEC 4561 Hardware and Software Interface, ELEC 2531 Logic Lab.</li></ul></div></div></div><div class="h-48 bg-gray-200 text-center pt-16 text-sm text-gray-500"><div>© 2022 Yan Pang. All rights reserved</div><div>(Last update: Aug 15, 2022.)</div></div></div></main></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/index_old/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-6a71d9fc662cbda582e6.js"],"app":["/app-f4756d18539972e693c4.js"],"component---src-pages-404-js":["/component---src-pages-404-js-00e77064645ae1056ad6.js"],"component---src-pages-index-js":["/component---src-pages-index-js-f71270d618976512c393.js"],"component---src-pages-index-old-js":["/component---src-pages-index-old-js-550796386d2c7103a1d8.js"]};/*]]>*/</script><script src="/polyfill-6a71d9fc662cbda582e6.js" nomodule=""></script><script src="/component---src-pages-index-old-js-550796386d2c7103a1d8.js" async=""></script><script src="/app-f4756d18539972e693c4.js" async=""></script><script src="/framework-5aba6dc98ac29269aae0.js" async=""></script><script src="/webpack-runtime-eeb24cc78a6f6e7e0a6a.js" async=""></script></body></html>